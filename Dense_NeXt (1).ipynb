{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VJqwTvH6hqej"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as cp\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class EnhancedInceptionModule(nn.Module):\n",
        "    def __init__(self, input_data_depth, output_data_depth, number_of_convolution_filters=32, max_kernel_size=7,\n",
        "                 dimensions_of_convolution=2):\n",
        "        super().__init__()\n",
        "        # The Member values are functions used in the inception, the convolution, max pool, and 1x1 convolution\n",
        "        # each made with three versions to handle one, two, and three dimensional data,\n",
        "\n",
        "        self.input_data_depth = input_data_depth\n",
        "        self.output_data_depth = output_data_depth\n",
        "        self.number_of_convolution_filters = number_of_convolution_filters\n",
        "        self.max_kernel_size = max_kernel_size\n",
        "        self.dimensions_of_convolution = dimensions_of_convolution\n",
        "\n",
        "        # Convolution layers,\n",
        "\n",
        "        self.convolution_1d = nn.Conv1d(in_channels=number_of_convolution_filters,\n",
        "                                        out_channels=number_of_convolution_filters, kernel_size=2, padding='same',\n",
        "                                        bias=False)\n",
        "        self.convolution_2d = nn.Conv2d(in_channels=number_of_convolution_filters,\n",
        "                                        out_channels=number_of_convolution_filters, kernel_size=2, padding='same',\n",
        "                                        bias=False)\n",
        "        self.convolution_3d = nn.Conv3d(in_channels=number_of_convolution_filters,\n",
        "                                        out_channels=number_of_convolution_filters, kernel_size=2, padding='same',\n",
        "                                        bias=False)\n",
        "\n",
        "        # 1x1 Convolution layers,\n",
        "\n",
        "        self.convolution_1d_1x1 = nn.Conv1d(in_channels=input_data_depth,\n",
        "                                            out_channels=number_of_convolution_filters, kernel_size=1, padding='same',\n",
        "                                             bias=False)\n",
        "        self.convolution_2d_1x1 = nn.Conv2d(in_channels=input_data_depth,\n",
        "                                            out_channels=number_of_convolution_filters, kernel_size=1, padding='same',\n",
        "                                             bias=False)\n",
        "        self.convolution_3d_1x1 = nn.Conv3d(in_channels=input_data_depth,\n",
        "                                            out_channels=number_of_convolution_filters, kernel_size=1, padding='same',\n",
        "                                             bias=False)\n",
        "\n",
        "        # Max pooling layers,\n",
        "\n",
        "        self.max_pool_1d = nn.MaxPool1d(kernel_size=3, stride=1, padding=1)\n",
        "        self.max_pool_2d = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "        self.max_pool_3d = nn.MaxPool3d(kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.max_kernel_size = max_kernel_size\n",
        "        self.dimensions_of_convolution = dimensions_of_convolution\n",
        "\n",
        "    def forward(self, input_data):\n",
        "\n",
        "        if self.dimensions_of_convolution == 1:\n",
        "            convolution = self.convolution_1d\n",
        "            max_pool = self.max_pool_1d\n",
        "            convolution_1x1 = self.convolution_1d_1x1\n",
        "\n",
        "        elif self.dimensions_of_convolution == 2:\n",
        "            convolution = self.convolution_2d\n",
        "            max_pool = self.max_pool_2d\n",
        "            convolution_1x1 = self.convolution_2d_1x1\n",
        "\n",
        "        elif self.dimensions_of_convolution == 3:\n",
        "            convolution = self.convolution_3d\n",
        "            max_pool = self.max_pool_3d\n",
        "            convolution_1x1 = self.convolution_3d_1x1       \n",
        "        \n",
        "\n",
        "        # an else statement to handle invalid dimensions, if the variable dimensions_of_convolution\n",
        "        # is not equal to 1 or 2 or 3.\n",
        "\n",
        "        else:\n",
        "            convolution_dimensional_error = \"Invalid convolution dimensions.\"\n",
        "            return convolution_dimensional_error\n",
        "\n",
        "        convolution_1x1.in_channels = input_data.size(dim=1)\n",
        "\n",
        "        # The Max pool branch from the inception module (future reminder: put a link to the enhanced inception here)\n",
        "\n",
        "        convolution_input = convolution_1x1(input_data)\n",
        "        collective_data = convolution_1x1(input_data)\n",
        "\n",
        "        # Preparing the data for the iteration of convolution operation.\n",
        "\n",
        "        max_pool_output = max_pool(input_data)\n",
        "        convolution_1x1_output = convolution_1x1(max_pool_output)\n",
        "        collective_data = torch.cat((collective_data, convolution_1x1_output), 1)\n",
        "\n",
        "        # Iterating the convolution operation over the data (max_kernel_size - 1) times,\n",
        "        # Explanation:\n",
        "        #\n",
        "        # Tensor height/width after convolution = (height/width before - kernel height/width)/ stride + 1\n",
        "        # more concisely H/W_out = (H/W_in - H/W_kernel)/s +1\n",
        "        # at H/W_kernel = 2, s = 1\n",
        "        #\n",
        "        # H/W_out = ((H/W_in - 2)/ 1 + 1\n",
        "        #\n",
        "        # H/W_out = H/W_in - 1\n",
        "        #\n",
        "        # Therefore, to do a 7x7 convolution, where a 7x7 partition of the tensor is reduced\n",
        "        # to a single 1x1 square, requires 6 2x2 convolutions, and a 5x5 requires 4,\n",
        "        # a 3x3 requires 2.\n",
        "        #\n",
        "        # and a result, it is self-evident from the general pattern beforehand established,\n",
        "        # that a convolution of Kernel size K, requires K-1 (2x2) convolutions.\n",
        "        # ( or 1x2 or 2x2x2 for one-dimensional and three-dimensional convolutions.\n",
        "\n",
        "        for i in range(self.max_kernel_size - 1):\n",
        "          convolution.in_channels = collective_data.size(dim=1)\n",
        "          convolution_output = convolution(convolution_input)\n",
        "          collective_data = torch.cat((collective_data, convolution_output), 1)\n",
        "          convolution_input = convolution_output\n",
        "\n",
        "        # Applying 1x1 convolution to change the depth of the collective_data as to match the\n",
        "        # desired output depth.\n",
        "\n",
        "        if self.dimensions_of_convolution == 1:\n",
        "            output_1x1_convolution = nn.Conv1d(in_channels=collective_data.size(dim=2),\n",
        "                                               out_channels=self.output_data_depth, kernel_size=1, padding='same',\n",
        "                                               bias=False)\n",
        "            return output_1x1_convolution(collective_data)\n",
        "\n",
        "        elif self.dimensions_of_convolution == 2:\n",
        "            output_1x1_convolution = nn.Conv2d(in_channels=collective_data.size(dim=2),\n",
        "                                               out_channels=self.output_data_depth, kernel_size=1, padding='same',\n",
        "                                               bias=False)\n",
        "            return output_1x1_convolution(collective_data)\n",
        "\n",
        "        else:\n",
        "            output_1x1_convolution = nn.Conv3d(in_channels=collective_data.size(dim=2),\n",
        "                                               out_channels=self.output_data_depth, kernel_size=1, padding='same',\n",
        "                                               bias=False)\n",
        "            return output_1x1_convolution(collective_data)\n"
      ],
      "metadata": {
        "id": "EDOPzrk8k_hl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _batch_norm_function_factory(batch_norm, relu, convolution):\n",
        "    def batch_norm_function(*inputs):\n",
        "        concatenated_features = torch.cat(inputs, 1)\n",
        "        bottleneck_output = convolution(relu(batch_norm(concatenated_features)))\n",
        "        return bottleneck_output\n",
        "\n",
        "    return batch_norm_function"
      ],
      "metadata": {
        "id": "EX7TweMCiG1-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer(nn.Module):\n",
        "    def __init__(self, number_of_input_features, growth_rate, batch_norm_size, drop_rate,\n",
        "                 model_dimensions_of_convolution, efficient=False):\n",
        "        super(DenseLayer, self).__init__()\n",
        "        self.number_of_input_features = number_of_input_features\n",
        "        self.growth_rate = growth_rate\n",
        "        self.batch_norm_size = batch_norm_size\n",
        "        self.drop_rate = drop_rate\n",
        "        self.model_dimensions_of_convolution = model_dimensions_of_convolution\n",
        "        self.efficient = efficient\n",
        "        \n",
        "        self.add_module('batch_norm_1_1d', nn.BatchNorm1d(number_of_input_features)),\n",
        "        self.add_module('batch_norm_1_2d', nn.BatchNorm2d(number_of_input_features)),\n",
        "        self.add_module('batch_norm_1_3d', nn.BatchNorm3d(number_of_input_features)),\n",
        "\n",
        "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
        "\n",
        "        self.add_module('convolution_1d_1x1', nn.Conv1d(number_of_input_features, batch_norm_size * growth_rate,\n",
        "                                                        kernel_size=1, stride=1, bias=False)),\n",
        "        self.add_module('convolution_2d_1x1', nn.Conv2d(number_of_input_features, batch_norm_size * growth_rate,\n",
        "                                                        kernel_size=1, stride=1, bias=False)),\n",
        "        self.add_module('convolution_3d_1x1', nn.Conv3d(number_of_input_features, batch_norm_size * growth_rate,\n",
        "                                                        kernel_size=1, stride=1, bias=False)),\n",
        "\n",
        "        self.add_module('batch_norm_2_1d', nn.BatchNorm1d(batch_norm_size * growth_rate)),\n",
        "        self.add_module('batch_norm_2_2d', nn.BatchNorm2d(batch_norm_size * growth_rate)),\n",
        "        self.add_module('batch_norm_2_3d', nn.BatchNorm3d(batch_norm_size * growth_rate)),\n",
        "\n",
        "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
        "\n",
        "        self.add_module('inception',\n",
        "                        EnhancedInceptionModule(\n",
        "                            input_data_depth=batch_norm_size * growth_rate,\n",
        "                            output_data_depth=growth_rate,\n",
        "                            number_of_convolution_filters=16,\n",
        "                            max_kernel_size=11,\n",
        "                            dimensions_of_convolution=model_dimensions_of_convolution)),\n",
        "        self.drop_rate = drop_rate\n",
        "        self.efficient = efficient\n",
        "        self.model_dimensions_of_convolution = model_dimensions_of_convolution\n",
        "\n",
        "    def forward(self, *previous_features):\n",
        "\n",
        "        batch_norm_function = _batch_norm_function_factory(batch_norm=self.batch_norm_1_2d,\n",
        "                                                           relu=self.relu1,\n",
        "                                                           convolution=self.convolution_2d_1x1)\n",
        "\n",
        "        if self.model_dimensions_of_convolution == 1:\n",
        "            batch_norm_function = _batch_norm_function_factory(batch_norm=self.batch_norm_1_1d,\n",
        "                                                               relu=self.relu1,\n",
        "                                                               convolution=self.convolution_1d_1x1)\n",
        "        elif self.model_dimensions_of_convolution == 3:\n",
        "            batch_norm_function = _batch_norm_function_factory(batch_norm=self.batch_norm_1_3d,\n",
        "                                                               relu=self.relu1,\n",
        "                                                               convolution=self.convolution_3d_1x1)\n",
        "\n",
        "        if self.efficient and any(previous_feature.requires_grad for previous_feature in previous_features):\n",
        "            bottleneck_output = cp.checkpoint(batch_norm_function, *previous_features)\n",
        "        else:\n",
        "            bottleneck_output = batch_norm_function(*previous_features)\n",
        "\n",
        "        new_features = self.inception(self.relu2(self.batch_norm_2_2d(bottleneck_output)))\n",
        "        \n",
        "        if self.model_dimensions_of_convolution == 1:\n",
        "            new_features = self.inception(self.relu2(self.batch_norm_2_1d(bottleneck_output)))\n",
        "            \n",
        "        elif self.model_dimensions_of_convolution == 3:\n",
        "            new_features = self.inception(self.relu2(self.batch_norm_2_3d(bottleneck_output)))\n",
        "           \n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
        "        return new_features"
      ],
      "metadata": {
        "id": "uuBguvrRiOPG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class _Transition(nn.Module):\n",
        "    def __init__(self, number_of_input_features, number_of_output_features, model_dimensions_of_convolution):\n",
        "        super(_Transition, self).__init__()\n",
        "        self.number_of_input_features = number_of_input_features\n",
        "        self.number_of_output_features = number_of_output_features\n",
        "        self.model_dimensions_of_convolution = model_dimensions_of_convolution\n",
        "\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\n",
        "\n",
        "        self.add_module('convolution_1x1_1d', nn.Conv1d(number_of_input_features, number_of_output_features,\n",
        "                                                        kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module('convolution_1x1_2d', nn.Conv2d(number_of_input_features, number_of_output_features,\n",
        "                                                        kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module('convolution_1x1_3d', nn.Conv3d(number_of_input_features, number_of_output_features,\n",
        "                                                        kernel_size=1, stride=1, bias=False))\n",
        "\n",
        "        self.add_module('average_pool_1d', nn.AvgPool1d(kernel_size=2, stride=2))\n",
        "        self.add_module('average_pool_2d', nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "        self.add_module('average_pool_3d', nn.AvgPool3d(kernel_size=2, stride=2))\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        if self.model_dimensions_of_convolution == 1:\n",
        "          transition_batch_norm = nn.BatchNorm1d(input_data.size(dim=1))\n",
        "          return self.average_pool_1d(self.convolution_1x1_1d(self.relu(transition_batch_norm(input_data))))\n",
        "        elif self.model_dimensions_of_convolution == 2:\n",
        "          transition_batch_norm = nn.BatchNorm1d(input_data.size(dim=1))\n",
        "          return self.average_pool_2d(self.convolution_1x1_2d(self.relu(transition_batch_norm(input_data))))\n",
        "        elif self.model_dimensions_of_convolution == 3:\n",
        "          transition_batch_norm = nn.BatchNorm1d(input_data.size(dim=1))\n",
        "          return self.average_pool_3d(self.convolution_1x1_3d(self.relu(transition_batch_norm(input_data))))\n",
        "        # an else statement to handle invalid dimensions, if the variable dimensions_of_convolution\n",
        "        # is not equal to 1 or 2 or 3.\n",
        "\n",
        "        else:\n",
        "            convolution_dimensional_error = \"Invalid convolution dimensions.\"\n",
        "            return convolution_dimensional_error"
      ],
      "metadata": {
        "id": "bkSmcgkhiSGd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class _DenseBlock(nn.Module):\n",
        "    def __init__(self, number_of_layers, number_of_input_features, batch_norm_size, growth_rate,\n",
        "                 drop_rate, model_dimensions_of_convolution, efficient=False):\n",
        "        super(_DenseBlock, self).__init__()\n",
        "        self.number_of_layers = number_of_layers\n",
        "        self.number_of_input_features = number_of_input_features\n",
        "        self.batch_norm_size = batch_norm_size\n",
        "        self.growth_rate = growth_rate\n",
        "        self.drop_rate = drop_rate\n",
        "        self.model_dimensions_of_convolution = model_dimensions_of_convolution\n",
        "        self.efficient = efficient\n",
        "        self.dense_layer = DenseLayer(number_of_input_features=number_of_input_features,\n",
        "                                       growth_rate=growth_rate, batch_norm_size=batch_norm_size,\n",
        "                                       model_dimensions_of_convolution=model_dimensions_of_convolution,\n",
        "                                       drop_rate=drop_rate, efficient=efficient\n",
        "                                       )\n",
        "        self.number_of_layers = number_of_layers\n",
        "\n",
        "    def forward(self, initial_features):\n",
        "        features = [initial_features]\n",
        "        for name, layer in self.named_children():\n",
        "            new_features = layer(*features)\n",
        "            features.append(new_features)\n",
        "        return torch.cat(features, 1)\n",
        "\n",
        "    \"\"\"\n",
        "    def forward(self, initial_features):\n",
        "        features = torch.tensor(initial_features)\n",
        "        for i in range(self.number_of_layers):\n",
        "            new_features = self.dense_layer(features)\n",
        "            features = torch.cat((features, new_features), dim=1)\n",
        "            self.dense_layer.number_of_input_features += i * self.growth_rate\n",
        "        return features\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "6z0Yd9wAle4W"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionalClassifier(nn.Module):\n",
        "    def __init__(self, classifier_input_channels, number_of_classes,\n",
        "                 model_dimensions_of_convolution, activation_function=nn.ReLU()):\n",
        "        super(ConvolutionalClassifier, self).__init__()\n",
        "\n",
        "        self.classifier_input_channels = classifier_input_channels\n",
        "        self.number_of_classes = number_of_classes\n",
        "        self.model_dimensions_of_convolution = model_dimensions_of_convolution\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "        assert 1 <= model_dimensions_of_convolution <= 3, 'model_dimensions_of_convolution should be between 1 and 3'\n",
        "\n",
        "        self.convolutional_classifier_1d = nn.Sequential(\n",
        "            nn.Conv1d(self.classifier_input_channels, 1024, kernel_size=1), activation_function,\n",
        "            nn.Conv1d(1024, 512, kernel_size=1), activation_function,\n",
        "            nn.Conv1d(512, 256, kernel_size=1), activation_function,\n",
        "            nn.Conv1d(256, 128, kernel_size=1), activation_function,\n",
        "            nn.Conv1d(128, number_of_classes, kernel_size=1), activation_function,\n",
        "        )\n",
        "        self.convolutional_classifier_2d = nn.Sequential(\n",
        "            nn.Conv2d(self.classifier_input_channels, 1024, kernel_size=1), activation_function,\n",
        "            nn.Conv2d(1024, 512, kernel_size=1), activation_function,\n",
        "            nn.Conv2d(512, 256, kernel_size=1), activation_function,\n",
        "            nn.Conv2d(256, 128, kernel_size=1), activation_function,\n",
        "            nn.Conv2d(128, number_of_classes, kernel_size=1), activation_function,\n",
        "        )\n",
        "        self.convolutional_classifier_3d = nn.Sequential(\n",
        "            nn.Conv3d(self.classifier_input_channels, 1024, kernel_size=1), activation_function,\n",
        "            nn.Conv3d(1024, 512, kernel_size=1), activation_function,\n",
        "            nn.Conv3d(512, 256, kernel_size=1), activation_function,\n",
        "            nn.Conv3d(256, 128, kernel_size=1), activation_function,\n",
        "            nn.Conv3d(128, number_of_classes, kernel_size=1), activation_function,\n",
        "        )\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        if self.model_dimensions_of_convolution == 1:\n",
        "            return self.convolutional_classifier_1d(input_data)\n",
        "        elif self.model_dimensions_of_convolution == 2:\n",
        "            return self.convolutional_classifier_2d(input_data)\n",
        "        else:\n",
        "            return self.convolutional_classifier_3d(input_data)\n",
        "\n",
        "\n",
        "class DenseNeXt(nn.Module):\n",
        "    r\"\"\"DenseNet-BC model class, based on\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n",
        "    Args:\n",
        "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
        "        dense_block_configuration (list of 3 or 4 ints) - how many layers in each pooling block\n",
        "        number_of_initial_features (int) - the number of filters to learn in the first convolution layer\n",
        "        batch_norm_size (int) - multiplicative factor for number of bottle neck layers\n",
        "            (i.e. batch_norm_size * k features in the bottleneck layer)\n",
        "        drop_rate (float) - dropout rate after each dense layer\n",
        "        number_of_classes (int) - number of classification classes\n",
        "        small_inputs (bool) - set to True if images are 32x32. Otherwise assumes images are larger.\n",
        "        efficient (bool) - set to True to use checkpointing. Much more memory efficient, but slower.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_data_depth=3, growth_rate=12, dense_block_configuration=(16, 16, 16), compression=0.5,\n",
        "                 number_of_initial_features=82, batch_norm_size=4, drop_rate=0, number_of_classes=10,\n",
        "                 model_dimensions_of_convolution=2, small_inputs=True, efficient=False, inception_convolution_filters=16\n",
        "                 ):\n",
        "        super(DenseNeXt, self).__init__()\n",
        "        assert 0 < compression <= 1, 'compression of DenseNeXt should be between 0 and 1'\n",
        "        assert 1 <= model_dimensions_of_convolution <= 3, 'model_dimensions_of_convolution should be between 1 and 3'\n",
        "\n",
        "        self.input_data_depth = input_data_depth\n",
        "        self.growth_rate = growth_rate\n",
        "        self.dense_block_configuration = dense_block_configuration\n",
        "        self.compression = compression\n",
        "        self.number_of_initial_features = number_of_initial_features\n",
        "        self.batch_norm_size = batch_norm_size\n",
        "        self.drop_rate = drop_rate\n",
        "        self.number_of_classes = number_of_classes\n",
        "        self.model_dimensions_of_convolution = model_dimensions_of_convolution\n",
        "        self.small_inputs = small_inputs\n",
        "        self.inception_convolution_filters = inception_convolution_filters\n",
        "        self.classifier_input_channels = 0\n",
        "\n",
        "        self.initial_Inception = EnhancedInceptionModule(input_data_depth=input_data_depth,\n",
        "                                           output_data_depth=number_of_initial_features,\n",
        "                                           number_of_convolution_filters=inception_convolution_filters,\n",
        "                                           dimensions_of_convolution=model_dimensions_of_convolution,\n",
        "                                           max_kernel_size=7\n",
        "                                           )\n",
        "        if model_dimensions_of_convolution == 1:\n",
        "            self.initial_batch_norm = nn.BatchNorm1d(number_of_initial_features)\n",
        "            self.initial_max_pool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1, ceil_mode=False)\n",
        "            self.final_batch_norm = nn.BatchNorm1d(number_of_initial_features)\n",
        "\n",
        "        elif model_dimensions_of_convolution == 2:\n",
        "            self.initial_batch_norm = nn.BatchNorm2d(number_of_initial_features)\n",
        "            self.initial_max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=False)\n",
        "            self.final_batch_norm = nn.BatchNorm2d(number_of_initial_features)\n",
        "        elif model_dimensions_of_convolution == 3:\n",
        "            self.initial_batch_norm = nn.BatchNorm3d(number_of_initial_features)\n",
        "            self.initial_max_pool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1, ceil_mode=False)\n",
        "            self.final_batch_norm = nn.BatchNorm3d(number_of_initial_features)\n",
        "\n",
        "        self.initial_ReLU = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.dense_block = _DenseBlock(\n",
        "            number_of_layers=dense_block_configuration[0],\n",
        "            number_of_input_features=number_of_initial_features,\n",
        "            batch_norm_size=batch_norm_size,\n",
        "            growth_rate=growth_rate,\n",
        "            drop_rate=drop_rate,\n",
        "            efficient=efficient,\n",
        "            model_dimensions_of_convolution=model_dimensions_of_convolution\n",
        "        )\n",
        "\n",
        "        self.transition = _Transition(\n",
        "            number_of_input_features=number_of_initial_features,\n",
        "            number_of_output_features=int(number_of_initial_features * self.compression),\n",
        "            model_dimensions_of_convolution=model_dimensions_of_convolution\n",
        "        )\n",
        "\n",
        "        if model_dimensions_of_convolution == 1:\n",
        "            self.final_batch_norm = nn.BatchNorm1d(number_of_initial_features)\n",
        "            self.classifier = ConvolutionalClassifier(0, number_of_classes=self.number_of_classes,\n",
        "                                                      model_dimensions_of_convolution=self.model_dimensions_of_convolution)\n",
        "        elif model_dimensions_of_convolution == 2:\n",
        "            self.final_batch_norm = nn.BatchNorm2d(number_of_initial_features)\n",
        "            self.classifier = ConvolutionalClassifier(0, number_of_classes=self.number_of_classes,\n",
        "                                                      model_dimensions_of_convolution=self.model_dimensions_of_convolution)\n",
        "        elif model_dimensions_of_convolution == 3:\n",
        "            self.final_batch_norm = nn.BatchNorm3d(number_of_initial_features)\n",
        "            self.classifier = ConvolutionalClassifier(0, number_of_classes=self.number_of_classes,\n",
        "                                                      model_dimensions_of_convolution=self.model_dimensions_of_convolution)\n",
        "\n",
        "        # TODO: try convolution as a classifier\n",
        "\n",
        "        for name, parameter in self.named_parameters():\n",
        "            if 'conv' in name and 'weight' in name:\n",
        "                dimensions = [parameter.size]\n",
        "                n = parameter.size(0)\n",
        "                for i in range(2, len(dimensions)):\n",
        "                    n *= parameter.size(i)\n",
        "                parameter.data.normal_().mul_(math.sqrt(2. / n))\n",
        "            elif 'norm' in name and 'weight' in name:\n",
        "                parameter.data.fill_(1)\n",
        "            elif 'norm' in name and 'bias' in name:\n",
        "                parameter.data.fill_(0)\n",
        "            elif 'classifier' in name and 'bias' in name:\n",
        "                parameter.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.initial_Inception(x)\n",
        "\n",
        "        if not self.small_inputs:\n",
        "            features = self.initial_max_pool(self.initial_batch_norm(features))\n",
        "\n",
        "        for i in range(len(self.dense_block_configuration)):\n",
        "            self.dense_block.number_of_layers = self.dense_block_configuration[i]\n",
        "            self.dense_block.number_of_input_features = features.size(dim=1)\n",
        "            features = self.dense_block(features)\n",
        "            \n",
        "            if i != len(self.dense_block_configuration) - 1: \n",
        "              self.transition.number_of_input_features = features.size(dim=1)\n",
        "              print(features.size(dim=1))\n",
        "              print(self.transition.number_of_input_features)\n",
        "              features = self.transition(features)\n",
        "        self.final_batch_norm.in_channels = features.size(dim=1)      \n",
        "        features = self.final_batch_norm(features)\n",
        "\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        self.classifier.classifier_input_channels = out.size(dim=1)\n",
        "        out = self.classifier(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        return out\n",
        "\n",
        "        # for i in dense_block_configuration:\n",
        "\n",
        "        # input_data_depth, output_data_depth, number_of_convolution_filters=32, max_kernel_size=7,\n",
        "        # dimensions_of_convolution=2\n",
        "\n",
        "    # def forward(self, x):\n"
      ],
      "metadata": {
        "id": "IAgYKLa3iYNN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = DenseNeXt()\n",
        "input_data = torch.randn(1, 3, 19, 11)\n",
        "print(net(input_data))\n",
        "# input_data_depth=3, growth_rate=12, dense_block_configuration=(16, 16, 16), compression=0.5,\n",
        "# number_of_initial_features=24, batch_norm_size=4, drop_rate=0, number_of_classes=10,\n",
        "# model_dimensions_of_convolution=2, small_inputs=True, efficient=False, inception_convolution_filters=16\n",
        "\n",
        "# number_of_input_features, growth_rate, batch_norm_size, drop_rate,\n",
        "# model_dimensions_of_convolution, efficient=False"
      ],
      "metadata": {
        "id": "fjYQk-zxixYm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "648262cd-a0c4-4e06-d4ab-efcc289930cd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ac3f059b7980>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenseNeXt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# input_data_depth=3, growth_rate=12, dense_block_configuration=(16, 16, 16), compression=0.5,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# number_of_initial_features=24, batch_norm_size=4, drop_rate=0, number_of_classes=10,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-08b53a5624d7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_Inception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmall_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-754627b15e05>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    125\u001b[0m                                                \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_data_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                                                bias=False)\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput_1x1_convolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollective_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    453\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 454\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [82, 19, 1, 1], expected input[1, 128, 19, 11] to have 19 channels, but got 128 channels instead"
          ]
        }
      ]
    }
  ]
}